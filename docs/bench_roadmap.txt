Benchmark Roadmap (JetStream + Arroyo/RisingWave + TiDB) Page 1
Lean Roadmap: NATS JetStream Benchmark
(Arroyo vs RisingWave)
Frozen plan and step-by-step build order • January 10, 2026
Scope
You are benchmarking a windowed aggregation pipeline using NATS JetStream for transport,
optionally Arroyo or RisingWave for processing, and TiDB as the sink. The goal is not to build a
production platform; it is to produce comparable end-to-end numbers (throughput, lag, p95/p99
latency) with minimal code.
Hard guardrails (keep it small)
• JetStream only (including bench.prod); no core NATS.
• Single Gradle Kotlin project. No frameworks (no Spring/DI, no metrics stacks).
• One file per runnable main. Print one metrics line per second (plus one startup line).
• Implement in order: compose -> streams -> Case A -> TiDB table -> Case B -> engines (C/D/E/F).
Stop when a step fails.
Frozen configuration
• Partitions p in {0..15} (16 subjects).
• Workload: processing-time 10s tumbling window GROUP BY key with aggregates count and
sum(value).
• Payload: 512 bytes fixed.
• Keys: num_keys = 20,000; key = "k" + (seq % num_keys).
• DB writer: batch_size_rows=500, flush_interval_ms=50, autocommit=false.
• Run timing: warmup 20s, measure 120s.
Cases (A-F) overview
Cas
e
Flow What it tells you
A K -> events.<p> -> K Transport sanity for raw events.* (JetStream +
client).
B Mock -> results.mock.<p> -> K writer ->
TiDB
Sink baseline: JetStream + writer + TiDB with
same output schema.
C K -> events.<p> -> Engine ->
results.<engine>.<p> -> K
Engine benchmark without DB (observe results
at NATS).
D K -> events.<p> -> Engine ->
results.<engine>.<p> -> K writer -> TiDB
End-to-end with Kotlin sink path.
E K -> events.<p> -> Arroyo ->
results.arroyo.<p> -> Benthos -> TiDB
End-to-end with Benthos sink path (Arroyo
only).
Benchmark Roadmap (JetStream + Arroyo/RisingWave + TiDB) Page 2
F K -> events.<p> -> RisingWave -> TiDB End-to-end with RisingWave direct sink (no
results subject).
Streams, subjects, durables
Streams
Stream Subjects Notes
EVENTS events.* 16 partitions: events.0 ... events.15
RESULTS results.* results.<variant>.<p>, where variant in {mock, arroyo,
rw}
CTRL bench.* bench.prod heartbeat messages
Durable consumer names (must match)
Cas
e
Durable Subscribes to
A dur.A.events events.*
B dur.B.results_mock results.mock.*
C dur.C.events.<engine> events.* (engine input)
C dur.C.results.<engine> results.<engine>.* (Kotlin observer)
D dur.D.events.<engine> events.* (engine input)
D dur.D.results.<engine> results.<engine>.* (Kotlin writer)
E dur.E.events.arroyo events.* (engine input)
E dur.E.results.arroyo results.arroyo.* (Benthos input)
F dur.F.events.rw events.* (engine input)
Benchmark Roadmap (JetStream + Arroyo/RisingWave + TiDB) Page 3
Schemas and metrics contracts
Event schema (events.)
• run_id
• event_id = run_id + "-" + seq
• seq (monotonic integer, producer-side)
• producer_ts_ms
• key = "k" + (seq % num_keys)
• value (long)
• payload_bytes (512B fixed)
Partitioning
p = hash(key) % 16; publish to events.<p>.
Result schema (results..)
• run_id
• key
• window_end_ms (end boundary of 10s tumbling window, epoch-aligned)
• count
• sum_value
• seq = max(input seq in that key/window) (progress marker; not contiguous)
• producer_ts_ms = max(input producer_ts_ms in that key/window)
Mock results (Case B)
Mock producer publishes the same result schema to results.mock.<p>. Use producer_ts_ms =
window_end_ms and seq as a simple global monotonic counter (start at 1, increment by 1 per
published result row).
bench.prod heartbeat (CTRL)
Published every 1s by the active producer (events producer for A/C/D/E/F, mock producer for B).
• run_id
• prod_max_seq
• prod_rate (msgs/s)
• producer_ts_ms_now
Observers keep the latest bench.prod per run_id.
TiDB table
Benchmark Roadmap (JetStream + Arroyo/RisingWave + TiDB) Page 4
bench.window_results columns:
• variant (mock/arroyo/rw)
• run_id, key, window_end_ms
• count, sum_value, seq, producer_ts_ms
• db_insert_ts (default now on insert/commit)
Primary key: (variant, run_id, key, window_end_ms) for idempotent inserts.
Latency and throughput definitions
• Primary latency: db_insert_ts - window_end_ms (end-to-end after window closes).
• Secondary latency: db_insert_ts - producer_ts_ms (sensitive to key sparsity; keep as supporting
signal).
• Lag: prod_max_seq - obs_max_seq.
• A/C obs_max_seq: max seq seen by Kotlin consumer (A from events seq, C from result seq field).
• B/D/E/F obs_max_seq: MAX(seq) from TiDB filtered by run_id and variant.
• Rates: Producer rate is events msgs/s; C observer rate is result rows/s; DB rate is committed
rows/s.
Benchmark Roadmap (JetStream + Arroyo/RisingWave + TiDB) Page 5
Build roadmap (stop at each gate)
Each step has a deliverable and a verification check. Do not move forward until it passes.
Step 1 - Bring up infrastructure
Do
• Start docker compose with NATS (JetStream enabled, memory storage) and TiDB.
• Ensure you can reach NATS and TiDB from host.
Verify
• docker compose up -d
• docker compose logs -n 50 nats
• docker compose logs -n 50 tidb
Step 2 - Create JetStream streams
Do
• Create streams: EVENTS (events.*), RESULTS (results.*), CTRL (bench.*).
• Keep storage=memory for all streams.
Verify
• nats stream add EVENTS --subjects "events.*" --storage memory
--retention limits --discard old --max-age -1 --no-confirm
• nats stream add RESULTS --subjects "results.*" --storage memory
--retention limits --discard old --max-age -1 --no-confirm
• nats stream add CTRL --subjects "bench.*" --storage memory --retention
limits --discard old --max-age -1 --no-confirm
• nats stream ls
Step 3 - Kotlin connectivity smoke test
Do
• Implement a tiny JetStream publish and subscribe test (one message) to bench.prod.
• Confirm the client can create a durable consumer.
Verify
• Run publisher once; run subscriber once; confirm it receives the message.
Step 4 - Case A (events transport baseline)
Do
Benchmark Roadmap (JetStream + Arroyo/RisingWave + TiDB) Page 6
• Run ProducerMain (events.* + bench.prod).
• Run ConsumerMain (dur.A.events on events.*).
• Verify metrics print once per second.
Verify
• Expect: obs_rate roughly matches prod_rate; lag stays bounded; p95/p99 stable.
Step 5 - TiDB schema
Do
• Create database/schema and bench.window_results table with PK (variant, run_id, key,
window_end_ms).
Verify
• Run a manual INSERT and SELECT COUNT(*) to confirm connectivity.
Step 6 - Case B (sink baseline)
Do
• Run MockResultsProducerMain (results.mock.* + bench.prod).
• Run WriterMain (dur.B.results_mock) to write to TiDB with batching.
• Run DB validator query loop (can be inside WriterMain) to print DB rows/s, lag, p95/p99.
Verify
• Expect: rows/s stable; lag bounded; table fills with variant=mock.
Step 7 - Engine bring-up (one at a time)
Do
• Deploy Arroyo OR RisingWave locally (docker is fine).
• Configure: input events.* (dur.C.events.<engine>), output results.<engine>.<p> (Case C only).
Verify
• Do not proceed until engine consumes events and produces results.
Step 8 - Case C (engine benchmark, no DB)
Do
• Run ProducerMain.
• Run engine producing results.<engine>.<p>.
• Run Kotlin ObserverMain consuming results.<engine>.* (dur.C.results.<engine>).
Benchmark Roadmap (JetStream + Arroyo/RisingWave + TiDB) Page 7
Verify
• Expect: observer sees result rows; lag shows a sawtooth pattern around 10s windows; no
unbounded growth.
Step 9 - Case D (engine + Kotlin writer + TiDB)
Do
• Reuse engine from Step 8.
• Run WriterMain consuming results.<engine>.* (dur.D.results.<engine>) and writing
variant=<engine>.
Verify
• Expect: DB rows/s stable; lag bounded.
Step 10 - Case E (Arroyo + Benthos + TiDB)
Do
• Run Arroyo producing results.arroyo.<p>.
• Run Benthos consuming results.arroyo.* (dur.E.results.arroyo) and writing to TiDB as
variant=arroyo.
Verify
• Expect: stable DB rows/s and lag.
Step 11 - Case F (RisingWave direct sink)
Do
• Configure RisingWave sink to TiDB (variant=rw).
• No results.rw.* subject required for F.
Verify
• Expect: rows appear in TiDB with variant=rw.
Step 12 - Benchmark run matrix
Do
• Run: A, B, C-arroyo, C-rw, D-arroyo, D-rw, E, F.
• For each: warmup 20s, measure 120s, save logs to a file.
• Capture docker stats for containers during measure window.
Verify
• Deliverable: one log file per run + a short summary table (peak rate, p95, p99, lag trend).
Benchmark Roadmap (JetStream + Arroyo/RisingWave + TiDB) Page 8
Benchmark Roadmap (JetStream + Arroyo/RisingWave + TiDB) Page 9
Metrics log line contract (copy/paste)
Print exactly one line per second per component during warmup and measure.
Producer
PROD rate=50000/s max_seq=1200000 run_id=...
Observer (Case A and Case C)
OBS rate=2000/s max_seq=1189000 lag=11000 p95=35ms p99=80ms run_id=...
DB validator (Case B/D/E/F)
DB rows=2000/s max_seq=1189000 lag=11000 p95=120ms p99=250ms run_id=...
Notes
• In A, rate is events/sec. In C, rate is result rows/sec.
• Lag in C uses output seq (max input seq per key/window), so it is a progress signal, not a
result-count signal.
• If lag grows across multiple windows without dropping, the pipeline is falling behind.

## Configuration

- Partitions: `p ∈ {0..15}` (16 subjects)
- Workload: processing-time **10s tumbling window** `GROUP BY key`, aggregates: `count`, `sum(value)`
- Payload: **512 bytes** fixed
- Keys:
  - `num_keys = 20_000`
  - `key = "k" + (seq % num_keys)`
- DB writer:
  - `batch_size_rows=500`
  - `flush_interval_ms=50`
  - `autocommit=false`
- Run timing:
  - warmup **20s**
  - measure **120s**

## Cases (A–F)

| Case | Flow | Purpose |
|---|---|---|
| A | `K -> events.<p> -> K` | Transport sanity (JetStream + client) |
| B | `Mock -> results.mock.<p> -> K writer -> TiDB` | Sink baseline (JetStream + writer + TiDB) |
| C | `K -> events.<p> -> Engine -> results.<engine>.<p> -> K` | Engine benchmark without DB (observe at NATS) |
| D | `K -> events.<p> -> Engine -> results.<engine>.<p> -> K writer -> TiDB` | End-to-end with Kotlin sink path |
| E | `K -> events.<p> -> Arroyo -> results.arroyo.<p> -> Benthos -> TiDB` | End-to-end with Benthos sink (Arroyo only) |
| F | `K -> events.<p> -> RisingWave -> TiDB` | End-to-end with RisingWave direct sink (no results subject) |

## Streams, subjects, durables

### Streams

| Stream | Subjects | Notes |
|---|---|---|
| EVENTS | `events.*` | 16 partitions: `events.0 ... events.15` |
| RESULTS | `results.*` | `results.<variant>.<p>`, `variant ∈ {mock, arroyo, rw}` |
| CTRL | `bench.*` | `bench.prod` heartbeat |

### Durable consumer names (must match)

| Case | Durable | Subscribes to |
|---|---|---|
| A | `dur.A.events` | `events.*` |
| B | `dur.B.results_mock` | `results.mock.*` |
| C | `dur.C.events.<engine>` | `events.*` (engine input) |
| C | `dur.C.results.<engine>` | `results.<engine>.*` (Kotlin observer) |
| D | `dur.D.events.<engine>` | `events.*` (engine input) |
| D | `dur.D.results.<engine>` | `results.<engine>.*` (Kotlin writer) |
| E | `dur.E.events.arroyo` | `events.*` (engine input) |
| E | `dur.E.results.arroyo` | `results.arroyo.*` (Benthos input) |
| F | `dur.F.events.rw` | `events.*` (engine input) |

## Schemas and metrics contracts

### Event schema (`events.<p>`)

- `run_id`
- `event_id = run_id + "-" + seq`
- `seq` (monotonic, producer-side)
- `producer_ts_ms`
- `key = "k" + (seq % num_keys)`
- `value` (long)
- `payload_bytes` (512B fixed)

### Partitioning

- `p = hash(key) % 16`
- Publish to `events.<p>`

### Result schema (`results.<variant>.<p>`)

- `run_id`
- `key`
- `window_end_ms` (end boundary of 10s tumbling window, epoch-aligned)
- `count`
- `sum_value`
- `seq = max(input seq in that key/window)` (progress marker; not contiguous)
- `producer_ts_ms = max(input producer_ts_ms in that key/window)`

### Mock results (Case B)

- Publish result schema to `results.mock.<p>`
- `producer_ts_ms = window_end_ms`
- `seq`: global monotonic counter (start at 1; increment by 1 per published result row)

### `bench.prod` heartbeat (CTRL)

- Publish every 1s by active producer
  - Events producer: A/C/D/E/F
  - Mock producer: B
- Fields:
  - `run_id`
  - `prod_max_seq`
  - `prod_rate` (msgs/s)
  - `producer_ts_ms_now`
- Observers: keep latest `bench.prod` per `run_id`

## TiDB table

### `bench.window_results`

Columns:

- `variant` (`mock` / `arroyo` / `rw`)
- `run_id`, `key`, `window_end_ms`
- `count`, `sum_value`, `seq`, `producer_ts_ms`
- `db_insert_ts` (default now on insert/commit)

Primary key:

- `(variant, run_id, key, window_end_ms)` (idempotent inserts)

## Metrics definitions

- Primary latency: `db_insert_ts - window_end_ms`
- Secondary latency: `db_insert_ts - producer_ts_ms`
- Lag: `prod_max_seq - obs_max_seq`
- `obs_max_seq` sources:
  - A: max `events.seq` seen by Kotlin consumer
  - C: max `results.seq` seen by Kotlin consumer
  - B/D/E/F: `MAX(seq)` in TiDB filtered by `run_id` + `variant`
- Rates:
  - Producer rate: events msgs/s
  - Case C observer rate: result rows/s
  - DB rate: committed rows/s

---

## Build roadmap (stop at each gate)

### Step 1 — Bring up infrastructure

Do
- Start docker compose: NATS (JetStream enabled, memory storage) + TiDB
- Host reachability to NATS + TiDB

Verify
```bash
docker compose up -d
docker compose logs -n 50 nats
docker compose logs -n 50 tidb
```

### Step 2 — Create JetStream streams

Do
- Create: EVENTS (`events.*`), RESULTS (`results.*`), CTRL (`bench.*`)
- `storage=memory` for all streams

Verify
```bash
nats stream add EVENTS --subjects "events.*" --storage memory   --retention limits --discard old --max-age -1 --no-confirm
nats stream add RESULTS --subjects "results.*" --storage memory   --retention limits --discard old --max-age -1 --no-confirm
nats stream add CTRL --subjects "bench.*" --storage memory   --retention limits --discard old --max-age -1 --no-confirm
nats stream ls
```

### Step 3 — Kotlin connectivity smoke test

Do
- Publish 1 JetStream message to `bench.prod`
- Subscribe 1 message from `bench.prod`
- Durable consumer creation

Verify
- Publisher once
- Subscriber once
- Message received

### Step 4 — Case A (events transport baseline)

Do
- Run `ProducerMain` (`events.*` + `bench.prod`)
- Run `ConsumerMain` (`dur.A.events` on `events.*`)
- Metrics line per second

Verify
- `obs_rate` ~ `prod_rate`
- lag bounded
- p95/p99 stable

### Step 5 — TiDB schema

Do
- Create database/schema
- Create `bench.window_results` with PK `(variant, run_id, key, window_end_ms)`

Verify
- Manual `INSERT`
- `SELECT COUNT(*)`

### Step 6 — Case B (sink baseline)

Do
- Run `MockResultsProducerMain` (`results.mock.*` + `bench.prod`)
- Run `WriterMain` (`dur.B.results_mock`) → TiDB (batching)
- DB validator loop (can be inside `WriterMain`): rows/s, lag, p95/p99

Verify
- rows/s stable
- lag bounded
- TiDB rows with `variant=mock`

### Step 7 — Engine bring-up (one at a time)

Do
- Deploy Arroyo **or** RisingWave (local; docker ok)
- Configure:
  - input: `events.*` via `dur.C.events.<engine>`
  - output (Case C): `results.<engine>.<p>`

Verify
- Engine consumes events
- Engine produces results

### Step 8 — Case C (engine benchmark, no DB)

Do
- Run `ProducerMain`
- Run engine → `results.<engine>.<p>`
- Run `ObserverMain` consuming `results.<engine>.*` (`dur.C.results.<engine>`)

Verify
- observer sees result rows
- lag sawtooth around 10s windows
- no unbounded growth

### Step 9 — Case D (engine + Kotlin writer + TiDB)

Do
- Reuse engine from Step 8
- Run `WriterMain` consuming `results.<engine>.*` (`dur.D.results.<engine>`)
- Write with `variant=<engine>`

Verify
- DB rows/s stable
- lag bounded

### Step 10 — Case E (Arroyo + Benthos + TiDB)

Do
- Run Arroyo → `results.arroyo.<p>`
- Run Benthos consuming `results.arroyo.*` (`dur.E.results.arroyo`)
- Write to TiDB with `variant=arroyo`

Verify
- DB rows/s stable
- lag bounded

### Step 11 — Case F (RisingWave direct sink)

Do
- Configure RisingWave sink → TiDB with `variant=rw`
- No `results.rw.*` subject

Verify
- TiDB rows with `variant=rw`

### Step 12 — Benchmark run matrix

Do
- Runs: A, B, C-arroyo, C-rw, D-arroyo, D-rw, E, F
- Per run:
  - warmup 20s
  - measure 120s
  - save logs to file
  - capture `docker stats` during measure window

Verify
- 1 log file per run
- Summary table: peak rate, p95, p99, lag trend

---

## Metrics log line contract

```text
Producer
PROD rate=50000/s max_seq=1200000 run_id=...

Observer (Case A and Case C)
OBS rate=2000/s max_seq=1189000 lag=11000 p95=35ms p99=80ms run_id=...

DB validator (Case B/D/E/F)
DB rows=2000/s max_seq=1189000 lag=11000 p95=120ms p99=250ms run_id=...
```

## Notes

- Case A: rate = events/sec
- Case C: rate = result rows/sec
- Case C lag uses output `seq` (max input seq per key/window)
- Lag grows across multiple windows without dropping → falling behind
